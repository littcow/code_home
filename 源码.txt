import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

app_train = pd.read_csv('application_train.csv')
app_test = pd.read_csv('application_test.csv')
print(app_train.shape)
print(app_test.shape)
print(app_train.head(10))  # 查看前10行数据

print(app_train['TARGET'].value_counts())

app_train['TARGET'].astype(int).plot.hist()
plt.show()

# 按列计算缺失值
def missing_values_table(df):
    # 总缺失值
    mis_val = df.isnull().sum()
    # 缺失值百分比
    mis_val_parent = 100*df.isnull().sum()/len(df)
    # 用结果做一个表格
    mis_val_table = pd.concat([mis_val, mis_val_parent], axis=1)
    # 重命名列
    mis_val_table_ren_columns = mis_val_table.rename(
        columns={0: 'Missing Values', 1: '% of Total Values'}
    )
    # 按丢失降序排列的百分比对表进行排序
    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
    print('Your selected dataframe has ' + str(df.shape[1]) + ' columns.\n'
          'There are ' + str(mis_val_table_ren_columns.shape[0]) + ' columns that have missing values.')
    return mis_val_table_ren_columns

# 缺失值统计
miss_values = missing_values_table(app_train)
print(miss_values.head(11)) 

print(app_train.dtypes.value_counts())
print(app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0))

# 创建一个LabelEncoder对象
le = LabelEncoder()
le_count = 0
# 标签编码
for col in app_train:
    if app_train[col].dtype == 'object':
        if len(app_train[col].unique()) <= 2:
            le.fit(app_train[col])
            app_train[col] = le.transform(app_train[col])
            app_test[col] = le.transform(app_test[col])
            le_count += 1
print('%d columns were label encoded.' % le_count)

app_train=app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)

train = pd.get_dummies(app_train)
test = pd.get_dummies(app_test)
print('Training Features shape: ', train.shape)
print('Testing Features shape: ', test.shape)

# train_labels = train['TARGET']
# app_train, app_test = train.align(test, join = 'inner', axis = 1)
# app_train['TARGET'] = train_labels
# print('Training Features shape: ', app_train.shape)
# print('Testing Features shape: ', app_test.shape)

# # 检查异常值
# print((train['DAYS_BIRTH'] / -365).describe())
# print(train['DAYS_EMPLOYED'].describe())

# anom = train[train['DAYS_EMPLOYED'] == 365243]
# non_anom = train[train['DAYS_EMPLOYED'] != 365243]
# print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))
# print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))
# print('There are %d anomalous days of employment' % len(anom))

# # 创建一个异常标志列
# train['DAYS_EMPLOYED_ANOM'] = train['DAYS_EMPLOYED'] == 365243
# # 用nan代替异常值
# train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)
# test['DAYS_EMPLOYED_ANOM'] = test['DAYS_EMPLOYED'] == 365243
# test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)
# print('There are %d anomalies in the test data out of %d entries' % (test["DAYS_EMPLOYED_ANOM"].sum(), len(test)))


# 查看与TARGET的相关系数并排序
correlations = train.corr()['TARGET'].sort_values()
# 显示相关系数
print('Most Positive Correlations:\n', correlations.tail(15))
print('\nMost Negative Correlations:\n', correlations.head(15))

# 找出DAYS_BIRTH和TARGET之间的相关系数
app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])
print(app_train['DAYS_BIRTH'].corr(app_train['TARGET']))

plt.figure(figsize=(10, 8))
# KDE图中按时偿还的贷款
sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / -365, label = 'target == 0')
# KDE图中未按时偿还的贷款
sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / -365, label = 'target == 1')
plt.xlabel('Age (years)')
plt.ylabel('Density')
plt.title('Distribution of Ages')
plt.show()

ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]
ext_data_corrs = ext_data.corr()
print(ext_data_corrs)

# 为多项式特征创建一个新的dataframe
poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]
poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]

imputer = SimpleImputer(strategy='median')

poly_target = poly_features['TARGET']
poly_features = poly_features.drop(columns=['TARGET'])
poly_features = imputer.fit_transform(poly_features)
poly_features_test = imputer.transform(poly_features_test)
poly_transformer = PolynomialFeatures(degree=3)

poly_transformer.fit(poly_features)

poly_features = poly_transformer.transform(poly_features)
poly_features_test = poly_transformer.transform(poly_features_test)
print('Polynomial Features shape: ', poly_features.shape)

print(poly_transformer.get_feature_names_out(input_features=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])
      [:15])

poly_features = pd.DataFrame(poly_features,
                             columns = poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2',
                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))

poly_features['TARGET'] = poly_target

# 找出与target的相关性
poly_corrs = poly_features.corr()['TARGET'].sort_values()

# 显示最消极和最积极的一面
print(poly_corrs.head(10))
print(poly_corrs.tail(5))

poly_features_test = pd.DataFrame(poly_features_test,
                                  columns = poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2',
                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))

poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']
app_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')

poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']
app_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')

app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)

print('Training data with polynomial features shape: ', app_train_poly.shape)
print('Testing data with polynomial features shape:  ', app_test_poly.shape)


app_train_domain = app_train.copy()
app_test_domain = app_test.copy()

app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']
app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']
app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']
app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']

app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']
app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']
app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']
app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']

# 逻辑回归
# 从训练集中删除TARGET列
if 'TARGET' in app_train:
    train = app_train.drop(columns = ['TARGET'])
else:
    train = app_train.copy()

features = list(train.columns)

test = app_test.copy()

# 缺失值的中值估计
imputer = SimpleImputer(strategy='median')

# 将每个特性缩放到0-1
scaler = MinMaxScaler(feature_range=(0, 1))

imputer.fit(train)

train = imputer.transform(train)
test = imputer.transform(app_test)

scaler.fit(train)
train = scaler.transform(train)
test = scaler.transform(test)

print('Training data shape: ', train.shape)
print('Testing data shape: ', test.shape)

log_reg = LogisticRegression(C = 0.0001)
log_reg.fit(train, train_labels)
log_reg_pred = log_reg.predict_proba(test)[:, 1]

submit = app_test[['SK_ID_CURR']]
submit['TARGET'] = log_reg_pred
# 将提交文件保存到csv文件中
submit.to_csv('log_reg_baseline.csv', index=False)


